- name: Deploy Longhorn Distributed Storage
  hosts: controlplane[0]
  gather_facts: false
  vars:
    cluster_config: "{{ lookup('file', 'tmp/{{ cluster_name }}/cluster_config.json') | from_json }}"
    longhorn_version: "{{ lookup('env', 'LONGHORN_VERSION') }}"
    kubeconfig_file: "{{ lookup('env', 'HOME') }}/.kube/{{ cluster_config.kubeconfig_file_name }}"
  environment:
    KUBECONFIG: "{{ kubeconfig_file }}"
  tasks:
    - name: Disable multipath daemon on all nodes to prevent Longhorn conflicts
      ansible.builtin.shell: |
        systemctl stop multipathd.socket multipathd.service || true
        systemctl disable multipathd.socket multipathd.service || true
        systemctl mask multipathd.service || true
        touch /etc/longhorn-multipath-disabled
      delegate_to: "{{ item }}"
      loop: "{{ groups['all'] }}"
      become: true

    - name: Disable Longhorn scheduling on GPU nodes
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Node
          metadata:
            name: "{{ item }}"
            labels:
              node.longhorn.io/create-default-disk: "false"
      loop: "{{ groups['gpu'] | default([]) }}"
      delegate_to: localhost
      when: groups['gpu'] is defined and groups['gpu'] | length > 0

    - name: Add Longhorn Helm repository
      kubernetes.core.helm_repository:
        name: longhorn
        repo_url: https://charts.longhorn.io
      delegate_to: localhost

    - name: Create longhorn-system namespace
      kubernetes.core.k8s:
        name: longhorn-system
        api_version: v1
        kind: Namespace
        state: present
      delegate_to: localhost

    - name: Install Longhorn using Helm
      kubernetes.core.helm:
        name: longhorn
        chart_ref: longhorn/longhorn
        release_namespace: longhorn-system
        create_namespace: true
        values:
          image:
            longhorn:
              engine:
                tag: "v{{ longhorn_version }}"
              manager:
                tag: "v{{ longhorn_version }}-hotfix-1"  # Use hotfix image to avoid known v1.10.0 bugs
              ui:
                tag: "v{{ longhorn_version }}"
              instanceManager:
                tag: "v{{ longhorn_version }}"
              shareManager:
                tag: "v{{ longhorn_version }}"
              backingImageManager:
                tag: "v{{ longhorn_version }}"
              supportBundleKit:
                tag: "v{{ longhorn_version }}"
          persistence:
            defaultClass: true
            defaultClassReplicaCount: 3
            defaultDataLocality: best-effort
          defaultSettings:
            defaultDataPath: "/var/lib/longhorn/"
            replicaSoftAntiAffinity: true
            createDefaultDiskLabeledNodes: true
            defaultLonghornStaticStorageClass: longhorn
            backupstorePollInterval: 300
            failedBackupTTL: 1440
            restoreVolumeRecurringJobs: true
            concurrentAutomaticEngineUpgradePerNodeLimit: 3
            systemManagedComponentsNodeSelector: "kubernetes.io/os:linux"
            # Conservative settings for gamma 5-disk storage
            storageOverProvisioningPercentage: 110   # Conservative over-provisioning (10% buffer)
            storageMinimalAvailablePercentage: 15    # Keep 15% free space minimum
            upgradeChecker: false                    # Disable upgrade checker for stability
            defaultReplicaCount: 3                   # Maintain 3-way replication for safety
            allowRecurringJobWhileVolumeDetached: true
            concurrentReplicaRebuildPerNodeLimit: 5  # Allow more concurrent rebuilds
            concurrentVolumeBackupRestorePerNodeLimit: 5
          longhornManager:
            nodeSelector:
              kubernetes.io/os: "linux"
          longhornDriver:
            nodeSelector:
              kubernetes.io/os: "linux"
          longhornUI:
            nodeSelector:
              kubernetes.io/os: "linux"
      delegate_to: localhost

    - name: Patch longhorn-manager to fix known v1.10.0 issues
      kubernetes.core.k8s:
        state: patched
        api_version: apps/v1
        kind: DaemonSet
        name: longhorn-manager
        namespace: longhorn-system
        definition:
          spec:
            template:
              spec:
                containers:
                  - name: longhorn-manager
                    # Fix readiness probe to check port 9502 (admission webhook) instead of 9501 (conversion webhook)
                    # The conversion webhook (port 9501) has a circular dependency and doesn't start
                    # This would cause the API server to continuously try to reach the webhook, consuming
                    # massive amounts of memory (50%+ of control plane RAM) and causing cluster instability
                    readinessProbe:
                      httpGet:
                        path: /v1/healthz
                        port: 9502
                        scheme: HTTPS
                      periodSeconds: 10
                      timeoutSeconds: 1
                      successThreshold: 1
                      failureThreshold: 3
                    # Remove --upgrade-version-check flag to prevent waitForOldLonghornManagersToBeFullyRemoved deadlock
                    command:
                      - longhorn-manager
                      - -d
                      - daemon
                      - --engine-image
                      - longhornio/longhorn-engine:v{{ longhorn_version }}
                      - --instance-manager-image
                      - longhornio/longhorn-instance-manager:v{{ longhorn_version }}
                      - --share-manager-image
                      - longhornio/longhorn-share-manager:v{{ longhorn_version }}
                      - --backing-image-manager-image
                      - longhornio/backing-image-manager:v{{ longhorn_version }}
                      - --support-bundle-manager-image
                      - longhornio/support-bundle-kit:v{{ longhorn_version }}
                      - --manager-image
                      - longhornio/longhorn-manager:v{{ longhorn_version }}-hotfix-1
                      - --service-account
                      - longhorn-service-account
      delegate_to: localhost

    - name: Patch longhorn-role ClusterRole to add missing RBAC permissions
      kubernetes.core.k8s:
        state: patched
        api_version: rbac.authorization.k8s.io/v1
        kind: ClusterRole
        name: longhorn-role
        definition:
          rules:
            # Add replicasets permission - required for csi-provisioner capacity-ownerref-level feature
            - apiGroups: ["apps"]
              resources: ["replicasets"]
              verbs: ["*"]
            # Add csistoragecapacities permission - required for csi-provisioner enable-capacity feature
            - apiGroups: ["storage.k8s.io"]
              resources: ["csistoragecapacities"]
              verbs: ["*"]
      delegate_to: localhost

    - name: Remove conversion webhook configuration from Longhorn CRDs
      kubernetes.core.k8s:
        state: patched
        api_version: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        name: "{{ item }}"
        definition:
          spec:
            conversion: null
      loop:
        - volumes.longhorn.io
        - nodes.longhorn.io
        - engineimages.longhorn.io
        - backuptargets.longhorn.io
        - replicas.longhorn.io
        - engines.longhorn.io
        - instancemanagers.longhorn.io
        - sharemanagers.longhorn.io
      delegate_to: localhost
      ignore_errors: true  # Some CRDs may not have conversion configured

    - name: Wait for Longhorn to be ready
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        name: longhorn-manager
        namespace: longhorn-system
      register: longhorn_daemonset
      until: longhorn_daemonset.resources[0].status.numberReady == longhorn_daemonset.resources[0].status.desiredNumberScheduled
      retries: 60
      delay: 5
      delegate_to: localhost

    - name: Wait for Longhorn StorageClass to be created
      kubernetes.core.k8s_info:
        api_version: storage.k8s.io/v1
        kind: StorageClass
        name: longhorn
      register: longhorn_storageclass
      until: longhorn_storageclass.resources | length > 0
      retries: 30
      delay: 2
      delegate_to: localhost

    - name: Check for existing default storage class
      kubernetes.core.k8s_info:
        api_version: storage.k8s.io/v1
        kind: StorageClass
      register: storage_classes
      delegate_to: localhost

    - name: Remove default annotation from local-path storage class
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: local-path
            annotations:
              storageclass.kubernetes.io/is-default-class: "false"
      when: storage_classes.resources | selectattr('metadata.name', 'equalto', 'local-path') | list | length > 0
      delegate_to: localhost

    - name: Set Longhorn as default storage class
      kubernetes.core.k8s:
        state: present
        merge_type: merge
        definition:
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: longhorn
            annotations:
              storageclass.kubernetes.io/is-default-class: "true"
      when: longhorn_storageclass.resources | length > 0
      delegate_to: localhost

    - name: Display Longhorn installation status
      debug:
        msg: |
          Longhorn v{{ longhorn_version }} has been successfully installed!
          
          Access Longhorn UI:
          kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80
          Then visit: http://localhost:8080
          
          Storage features:
          - Default StorageClass: longhorn (replicas=3)
          - Data locality: best-effort
          - Automatic disk discovery on labeled nodes
          - 1.5TB storage per general node (4.5TB total raw, ~1.5TB usable with 3x replication)
          
          For volume permission issues, use fsGroup in your pods:
          spec:
            securityContext:
              fsGroup: 1000  # Kubernetes handles volume ownership automatically
