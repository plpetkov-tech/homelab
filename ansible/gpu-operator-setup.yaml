---
- name: Install NVIDIA GPU Operator with Time-Slicing
  hosts: controlplane[0]
  gather_facts: false
  vars:
    cluster_config: "{{ lookup('file', 'tmp/{{ cluster_name }}/cluster_config.json') | from_json }}"
    longhorn_nodes: "{{ lookup('file', 'tmp/{{ cluster_name }}/longhorn_nodes.json') | from_json }}"
    gpu_operator_version: "{{ lookup('env', 'GPU_OPERATOR_VERSION') }}"
    time_slicing_replicas: "{{ lookup('env', 'NVIDIA_GPU_TIME_SLICING_REPLICAS') | default(4) }}"
    kubeconfig_file: "{{ lookup('env', 'HOME') }}/.kube/{{ cluster_config.kubeconfig_file_name }}"
  environment:
    KUBECONFIG: "{{ kubeconfig_file }}"
  tasks:
    - name: Check if there are GPU nodes in the cluster
      set_fact:
        gpu_nodes_exist: >-
          {{ cluster_config.node_classes.gpu is defined and cluster_config.node_classes.gpu.count > 0 }}

    - name: Configure containerd for NVIDIA Container Toolkit on GPU nodes
      shell: nvidia-ctk runtime configure --runtime=containerd
      become: true
      register: containerd_config_result
      delegate_to: "{{ item }}"
      loop: "{{ groups['gpu'] | default([]) }}"
      when: gpu_nodes_exist

    - name: Restart containerd on GPU nodes
      systemd:
        name: containerd
        state: restarted
      become: true
      delegate_to: "{{ item }}"
      loop: "{{ groups['gpu'] | default([]) }}"
      when: gpu_nodes_exist and containerd_config_result is changed

    - name: Create validation directories for external GPU drivers
      file:
        path: "{{ item.path }}"
        state: directory
        mode: '0755'
      become: true
      delegate_to: "{{ item.host }}"
      loop:
        - { host: "{{ groups['gpu'][0] | default('localhost') }}", path: "/run/nvidia/driver" }
        - { host: "{{ groups['gpu'][0] | default('localhost') }}", path: "/run/nvidia/validations" }
      when: gpu_nodes_exist

    - name: Create validation files for external GPU drivers
      file:
        path: "{{ item.path }}"
        state: touch
        mode: '0644'
      become: true
      delegate_to: "{{ item.host }}"
      loop:
        - { host: "{{ groups['gpu'][0] | default('localhost') }}", path: "/run/nvidia/validations/driver-ready" }
        - { host: "{{ groups['gpu'][0] | default('localhost') }}", path: "/run/nvidia/validations/toolkit-ready" }
      when: gpu_nodes_exist

    - name: Skip GPU Operator installation if no GPU nodes
      meta: end_play
      when: not gpu_nodes_exist

    - name: Display GPU Operator installation info
      debug:
        msg: |
          Installing NVIDIA GPU Operator {{ gpu_operator_version }}
          Time-slicing replicas: {{ time_slicing_replicas }}
          GPU nodes detected in cluster configuration

    - name: Check if GPU Operator is already installed
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Namespace
        name: gpu-operator
      register: gpu_operator_namespace
      failed_when: false
      delegate_to: localhost

    - name: Add NVIDIA Helm repository
      kubernetes.core.helm_repository:
        name: nvidia
        repo_url: https://helm.ngc.nvidia.com/nvidia
      delegate_to: localhost

    - name: Create gpu-operator namespace
      kubernetes.core.k8s:
        name: gpu-operator
        api_version: v1
        kind: Namespace
        state: present
      delegate_to: localhost

    - name: Create GPU time-slicing ConfigMap
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: time-slicing-config
            namespace: gpu-operator
          data:
            time-slicing-config: |
              version: v1
              flags:
                migStrategy: none
              sharing:
                timeSlicing:
                  resources:
                    - name: nvidia.com/gpu
                      replicas: {{ time_slicing_replicas }}
            any: |
              version: v1
              flags:
                migStrategy: none
              sharing:
                timeSlicing:
                  resources:
                    - name: nvidia.com/gpu
                      replicas: {{ time_slicing_replicas }}
      delegate_to: localhost

    - name: Install NVIDIA GPU Operator with time-slicing
      kubernetes.core.helm:
        name: gpu-operator
        chart_ref: nvidia/gpu-operator
        release_namespace: gpu-operator
        create_namespace: true
        values:
          operator:
            defaultRuntime: containerd
            resources:
              limits:
                cpu: 500m
                memory: 350Mi
              requests:
                cpu: 200m
                memory: 100Mi
          driver:
            enabled: false
          toolkit:
            enabled: true
            resources:
              limits:
                cpu: 500m
                memory: 350Mi
              requests:
                cpu: 200m
                memory: 100Mi
          devicePlugin:
            enabled: true
            config:
              name: time-slicing-config
              default: any
            resources:
              limits:
                cpu: 500m
                memory: 350Mi
              requests:
                cpu: 200m
                memory: 100Mi
          dcgmExporter:
            enabled: true
            resources:
              limits:
                cpu: 500m
                memory: 1Gi
              requests:
                cpu: 200m
                memory: 256Mi
          gfd:
            enabled: true
            resources:
              limits:
                cpu: 500m
                memory: 350Mi
              requests:
                cpu: 200m
                memory: 100Mi
          nodeStatusExporter:
            enabled: true
          migManager:
            enabled: false
          validator:
            plugin:
              env:
                - name: WITH_WORKLOAD
                  value: "true"
          nodeFeatureDiscovery:
            enabled: true
            worker:
              tolerations:
                - key: "gpu"
                  operator: "Equal"
                  value: "true"
                  effect: "NoSchedule"
                - key: "nvidia.com/gpu"
                  operator: "Exists"
                  effect: "NoSchedule"
              resources:
                limits:
                  cpu: 500m
                  memory: 350Mi
                requests:
                  cpu: 200m
                  memory: 100Mi
          sandboxWorkloads:
            enabled: false
        wait: false
        wait_timeout: 60
      delegate_to: localhost
      register: gpu_operator_install
      retries: 3
      delay: 30
      until: gpu_operator_install is succeeded

    - name: Display GPU Operator installation progress
      debug:
        msg: |
          GPU Operator installation initiated. This process can take 10-20 minutes depending on:
          - GPU driver compilation time
          - Container image downloads
          - Hardware initialization
          
          You can monitor progress with:
          kubectl get pods -n gpu-operator
          kubectl logs -n gpu-operator -l app=nvidia-driver-daemonset -f

    - name: Wait for GPU Operator namespace to be populated
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: gpu-operator
      register: gpu_pods
      until: gpu_pods.resources | length > 0
      retries: 30
      delay: 10
      delegate_to: localhost

    - name: Get all GPU operator DaemonSets
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        namespace: gpu-operator
      register: gpu_daemonsets_list
      delegate_to: localhost

    - name: Fix GPU operator DaemonSet node selectors and add tolerations
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: "{{ item.metadata.name }}"
            namespace: gpu-operator
          spec:
            template:
              spec:
                nodeSelector:
                  nodeclass: gpu
                tolerations: >-
                  {{
                    (item.spec.template.spec.tolerations | default([]))
                    + [{
                        'key': 'gpu',
                        'operator': 'Equal',
                        'value': 'true',
                        'effect': 'NoSchedule'
                      }]
                    + [{
                        'key': 'nvidia.com/gpu',
                        'operator': 'Exists',
                        'effect': 'NoSchedule'
                      }]
                  }}
        merge_type: merge
      loop: "{{ gpu_daemonsets_list.resources }}"
      when: "'driver' not in item.metadata.name"
      failed_when: false
      delegate_to: localhost

    - name: Fix driver DaemonSet with special handling
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: "{{ item.metadata.name }}"
            namespace: gpu-operator
          spec:
            template:
              spec:
                nodeSelector:
                  nodeclass: gpu
                tolerations: >-
                  {{
                    (item.spec.template.spec.tolerations | default([]))
                    + [{
                        'key': 'gpu',
                        'operator': 'Equal',
                        'value': 'true',
                        'effect': 'NoSchedule'
                      }]
                    + [{
                        'key': 'nvidia.com/gpu',
                        'operator': 'Exists',
                        'effect': 'NoSchedule'
                      }]
                  }}
        merge_type: merge
      loop: "{{ gpu_daemonsets_list.resources }}"
      when: "'driver' in item.metadata.name"
      failed_when: false
      delegate_to: localhost

    - name: Add ConfigMap permissions to GPU operator ServiceAccount Roles
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: rbac.authorization.k8s.io/v1
          kind: Role
          metadata:
            name: "{{ item }}"
            namespace: gpu-operator
          rules:
            - apiGroups: [""]
              resources: ["configmaps"]
              verbs: ["get", "list", "watch"]
        merge_type: merge
      loop:
        - nvidia-device-plugin
        - nvidia-gpu-feature-discovery
      failed_when: false
      delegate_to: localhost

    - name: Monitor GPU Operator components deployment
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        namespace: gpu-operator
      register: gpu_daemonsets
      until: gpu_daemonsets.resources | length > 0
      retries: 60
      delay: 10
      delegate_to: localhost

    - name: Display current GPU Operator pod status
      debug:
        msg: |
          GPU Operator Pods Status:
          {% for pod in gpu_pods.resources %}
          - {{ pod.metadata.name }}: {{ pod.status.phase | default('Unknown') }}
          {% endfor %}

    - name: Brief wait for GPU components to initialize
      pause:
        seconds: 30
        prompt: "Waiting for GPU operator components to initialize..."

    - name: Check GPU operator deployment status
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        namespace: gpu-operator
      register: gpu_final_status
      delegate_to: localhost

    - name: Display GPU operator DaemonSet status
      debug:
        msg: |
          DaemonSet: {{ item.metadata.name }}
          Desired: {{ item.status.desiredNumberScheduled | default(0) }}
          Ready: {{ item.status.numberReady | default(0) }}
          Status: {{ 'Ready' if item.status.numberReady == item.status.desiredNumberScheduled else 'Not Ready' }}
      loop: "{{ gpu_final_status.resources }}"
      when: item.status.desiredNumberScheduled | default(0) > 0

    - name: Check device plugin status
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        namespace: gpu-operator
        name: nvidia-device-plugin-daemonset
      register: device_plugin_status
      failed_when: false
      delegate_to: localhost

    - name: Display device plugin readiness
      debug:
        msg: |
          NVIDIA Device Plugin Status:
          {% if device_plugin_status.resources | length > 0 %}
          Ready Pods: {{ device_plugin_status.resources[0].status.numberReady | default(0) }}/{{ device_plugin_status.resources[0].status.desiredNumberScheduled | default(0) }}
          {% if device_plugin_status.resources[0].status.numberReady | default(0) == 0 %}
          
          ⚠️  Device plugin not ready yet. This is normal during initial GPU driver setup.
          Monitor progress: kubectl logs -n gpu-operator -l app=nvidia-device-plugin -f
          {% endif %}
          {% else %}
          Device plugin DaemonSet not found - GPU Operator may still be initializing
          {% endif %}

    - name: Check GPU nodes and verify time-slicing configuration
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu=true"
      register: gpu_labeled_nodes
      delegate_to: localhost

    - name: Get all GPU nodes for labeling
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nodeclass=gpu"
      register: gpu_nodes_info
      delegate_to: localhost
      when: gpu_nodes_exist

    - name: Apply time-slicing labels to actual GPU nodes
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Node
          metadata:
            name: "{{ item.metadata.name }}"
            labels:
              nvidia.com/device-plugin.config: "time-slicing-config"
              nvidia.com/gpu: "true"
        merge_type: merge
      loop: "{{ gpu_nodes_info.resources | default([]) }}"
      when: gpu_nodes_exist and gpu_nodes_info.resources | default([]) | length > 0
      delegate_to: localhost

    - name: Wait for time-slicing configuration to be applied
      pause:
        seconds: 30
        prompt: "Waiting for GPU time-slicing configuration to be applied and device plugin restart..."
      when: gpu_nodes_exist and gpu_nodes_info.resources | default([]) | length > 0
      
    - name: Restart GPU device plugin to apply time-slicing config
      kubernetes.core.k8s:
        state: absent
        api_version: v1
        kind: Pod
        namespace: gpu-operator
        label_selectors:
          - "app=nvidia-device-plugin-daemonset"
      delegate_to: localhost
      when: gpu_nodes_exist and gpu_nodes_info.resources | default([]) | length > 0
      ignore_errors: true
      
    - name: Wait for device plugin pods to be recreated
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: gpu-operator
        label_selectors:
          - "app=nvidia-device-plugin-daemonset"
        wait: true
        wait_condition:
          type: Ready
          status: "True"
        wait_timeout: 180
      delegate_to: localhost
      when: gpu_nodes_exist and gpu_nodes_info.resources | default([]) | length > 0
      register: device_plugin_restart
      failed_when: false

    - name: Verify GPU resources with time-slicing
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu=true"
      register: gpu_nodes_final
      delegate_to: localhost

    - name: Display GPU node information
      debug:
        msg: |
          GPU Node: {{ item.metadata.name }}
          Labels: {{ item.metadata.labels | dict2items | selectattr('key', 'match', 'nvidia.*') | map(attribute='key') | list }}
          Allocatable GPU Resources: {{ item.status.allocatable['nvidia.com/gpu'] | default('Not available') }}
          GPU Product: {{ item.metadata.labels.get('nvidia.com/gpu.product', 'Unknown') }}
      loop: "{{ gpu_nodes_final.resources }}"
      when: gpu_nodes_final.resources | length > 0

    - name: Check if GPU resources are available for testing
      set_fact:
        gpu_resources_ready: >-
          {{ gpu_nodes_final.resources | length > 0 and
             gpu_nodes_final.resources | selectattr('status.allocatable', 'defined') |
             selectattr('status.allocatable.nvidia.com/gpu', 'defined') | list | length > 0 }}
      when: gpu_nodes_exist

    - name: Check if GPU resources are now available after restart
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu=true"
      register: gpu_nodes_check
      delegate_to: localhost
      when: gpu_nodes_exist
      
    - name: Update GPU resources availability status
      set_fact:
        gpu_resources_ready: >-
          {{ gpu_nodes_check.resources | default([]) | length > 0 and
             gpu_nodes_check.resources | selectattr('status.allocatable', 'defined') |
             selectattr('status.allocatable.nvidia.com/gpu', 'defined') | list | length > 0 }}
      when: gpu_nodes_exist and gpu_nodes_check.resources is defined

    - name: Deploy fallback NVIDIA device plugin if GPU Operator fails
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: nvidia-device-plugin-fallback
            namespace: kube-system
            labels:
              app: nvidia-device-plugin-fallback
          spec:
            selector:
              matchLabels:
                name: nvidia-device-plugin-fallback
            template:
              metadata:
                labels:
                  name: nvidia-device-plugin-fallback
              spec:
                priorityClassName: system-node-critical
                tolerations:
                - key: gpu
                  operator: Equal
                  value: "true"
                  effect: NoSchedule
                - key: nvidia.com/gpu
                  operator: Exists
                  effect: NoSchedule
                - key: node.kubernetes.io/not-ready
                  operator: Exists
                  effect: NoExecute
                nodeSelector:
                  nodeclass: gpu
                containers:
                - image: nvcr.io/nvidia/k8s-device-plugin:v0.17.2
                  name: nvidia-device-plugin-ctr
                  args:
                  - "--device-discovery-strategy=nvml"
                  - "--fail-on-init-error=false"
                  - "--pass-device-specs=true"
                  env:
                  - name: NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  securityContext:
                    allowPrivilegeEscalation: false
                    capabilities:
                      drop: ["ALL"]
                  volumeMounts:
                  - name: device-plugin
                    mountPath: /var/lib/kubelet/device-plugins
                  - name: dev
                    mountPath: /dev
                  livenessProbe:
                    httpGet:
                      path: /healthz
                      port: 8080
                    initialDelaySeconds: 30
                    periodSeconds: 30
                volumes:
                - name: device-plugin
                  hostPath:
                    path: /var/lib/kubelet/device-plugins
                - name: dev
                  hostPath:
                    path: /dev
        wait: true
        wait_timeout: 120
      when: gpu_nodes_exist and not (gpu_resources_ready | default(false)) and device_plugin_restart.failed | default(false)
      delegate_to: localhost
      register: fallback_device_plugin
      
    - name: Wait for fallback device plugin to be ready
      kubernetes.core.k8s_info:
        api_version: apps/v1
        kind: DaemonSet
        name: nvidia-device-plugin-fallback
        namespace: kube-system
        wait: true
        wait_condition:
          type: "Available"
        wait_timeout: 180
      when: fallback_device_plugin is defined and fallback_device_plugin.changed
      delegate_to: localhost
      failed_when: false

    - name: Create GPU test workload (conditional)
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Pod
          metadata:
            name: gpu-test-pod
            namespace: default
            annotations:
              description: "GPU Operator validation test"
          spec:
            restartPolicy: Never
            containers:
            - name: gpu-test
              image: nvidia/cuda:12.0-runtime-ubuntu20.04
              command: ["sh", "-c", "nvidia-smi && echo 'GPU test completed successfully'"]
              resources:
                limits:
                  nvidia.com/gpu: 1
            tolerations:
            - key: "gpu"
              operator: "Equal"
              value: "true"
              effect: "NoSchedule"
            nodeSelector:
              nvidia.com/gpu: "true"
      when: gpu_nodes_exist and gpu_resources_ready | default(false)
      delegate_to: localhost

    - name: Wait for GPU test pod to complete with extended timeout
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        name: gpu-test-pod
        namespace: default
        wait: true
        wait_condition:
          type: "PodReadyCondition"
          status: "True"
        wait_timeout: 300
      register: gpu_test_result
      failed_when: false
      when: gpu_nodes_exist and gpu_resources_ready | default(false)
      delegate_to: localhost

    - name: Get GPU test pod logs
      kubernetes.core.k8s_log:
        name: gpu-test-pod
        namespace: default
      register: gpu_test_logs
      failed_when: false
      when: gpu_nodes_exist and gpu_resources_ready | default(false)
      delegate_to: localhost

    - name: Display GPU test results
      debug:
        msg: |
          {% if gpu_resources_ready | default(false) %}
          GPU Test Results:
          {{ gpu_test_logs.log if gpu_test_logs is defined and gpu_test_logs.log else 'GPU test pod logs not available' }}
          {% else %}
          ⚠️  GPU resources not yet available for testing.
          This is normal during initial GPU Operator setup.
          
          Manual validation commands:
          kubectl get nodes -l nvidia.com/gpu=true
          kubectl describe node <gpu-node-name>
          kubectl run gpu-test --rm -it --restart=Never --image=nvidia/cuda:12.0-runtime-ubuntu20.04 -- nvidia-smi
          {% endif %}
      when: gpu_nodes_exist

    - name: Clean up GPU test pod
      kubernetes.core.k8s:
        state: absent
        api_version: v1
        kind: Pod
        name: gpu-test-pod
        namespace: default
      failed_when: false
      when: gpu_nodes_exist and gpu_resources_ready | default(false)
      delegate_to: localhost

    - name: Final GPU operator status check
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Node
        label_selectors:
          - "nvidia.com/gpu=true"
      register: final_gpu_nodes
      delegate_to: localhost
      when: gpu_nodes_exist
      
    - name: Check GPU operator pod status
      kubernetes.core.k8s_info:
        api_version: v1
        kind: Pod
        namespace: gpu-operator
        field_selectors:
          - status.phase=Running
      register: running_gpu_pods
      delegate_to: localhost
      when: gpu_nodes_exist
      
    - name: Auto-remediation - Restart problematic GPU operator pods
      kubernetes.core.k8s:
        state: absent
        api_version: v1
        kind: Pod
        namespace: gpu-operator
        name: "{{ item.metadata.name }}"
      loop: "{{ (gpu_pods.resources | default([])) | selectattr('status.phase', 'eq', 'Failed') | list }}"
      delegate_to: localhost
      when: gpu_nodes_exist and gpu_pods.resources is defined
      ignore_errors: true
      
    # NOTE: Automatic fallback deployment disabled - GPU operator now works correctly with memory fix
    # Use 'ccr fix-gpu-operator --deploy-working' for manual troubleshooting if needed
    # - name: Deploy working GPU operator components as fallback
    #   include_tasks: gpu-operator-working-daemonsets-tasks.yaml
    #   when: gpu_nodes_exist and (final_gpu_nodes.resources | default([]) | length == 0 or final_gpu_nodes.resources | selectattr('status.allocatable.nvidia.com/gpu', 'defined') | list | length == 0)

    - name: Display GPU Operator installation summary
      debug:
        msg: |
          NVIDIA GPU Operator {{ gpu_operator_version }} installation completed!
          
          📊 Status Summary:
          - GPU Nodes Detected: {{ final_gpu_nodes.resources | default([]) | length }}
          - Running GPU Operator Pods: {{ running_gpu_pods.resources | default([]) | length }}
          - Time-slicing Replicas: {{ time_slicing_replicas }} per GPU
          - Fallback Device Plugin: {{ 'Deployed' if fallback_device_plugin is defined and fallback_device_plugin.changed else 'Not needed' }}
          
          {% if final_gpu_nodes.resources | default([]) | length > 0 %}
          ✅ GPU Resources Available:
          {% for node in final_gpu_nodes.resources %}
          - {{ node.metadata.name }}: {{ node.status.allocatable.get('nvidia.com/gpu', 'N/A') }} GPUs
          {% endfor %}
          {% else %}
          ⚠️  GPU resources not yet available. This may indicate:
          - GPU drivers are still compiling (can take 10-20 minutes)
          - GPU Operator pods are still initializing
          - Hardware detection issues
          {% endif %}
          
          📋 Monitoring Commands:
          - kubectl get pods -n gpu-operator
          - kubectl get nodes -l nvidia.com/gpu=true -o wide
          - kubectl describe node <gpu-node-name>
          
          🧪 Test GPU Functionality:
          - kubectl run gpu-test --rm -it --restart=Never --image=nvidia/cuda:12.0-runtime-ubuntu20.04 --overrides='{"spec":{"tolerations":[{"key":"gpu","operator":"Equal","value":"true","effect":"NoSchedule"}],"nodeSelector":{"nodeclass":"gpu"}}}' -- nvidia-smi
          
          📝 GPU Workload Template:
          spec:
            containers:
            - name: gpu-app
              resources:
                limits:
                  nvidia.com/gpu: 1  # Can be fractional with time-slicing
            tolerations:
            - key: \"gpu\"
              operator: \"Equal\" 
              value: \"true\"
              effect: \"NoSchedule\"
            nodeSelector:
              nodeclass: \"gpu\"
          
          🔄 GPU Operator will continue background operations. Working components deployed if needed.